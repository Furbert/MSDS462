{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Rendering OpenAi Gym in Colaboratory.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Furbert/MSDS462/blob/master/Copy_of_Rendering_OpenAi_Gym_in_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSGgH2Y0apl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO2p6UdNw8v4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "state.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyM56Dw0xAZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "env = gym.make(\"MsPacman-v0\")\n",
        "n_outputs = env.action_space.n\n",
        "print(n_outputs)\n",
        "print(env.env.get_action_meanings())\n",
        "observation = env.reset()\n",
        "\n",
        "for j in range(22):\n",
        "  if j > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "observation, _, _, _ = env.step(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u11-wilnxqqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def q_network(X, name_scope):\n",
        "# Initialize layers\n",
        "  initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n",
        "  with tf.compat.v1.variable_scope(name_scope) as scope:\n",
        "    # initialize the convolutional layers\n",
        "    layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer)\n",
        "    tf.compat.v1.summary.histogram('layer_1',layer_1)\n",
        "    layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4),    stride=2, padding='SAME', weights_initializer=initializer)\n",
        "    tf.compat.v1.summary.histogram('layer_2',layer_2)\n",
        "    layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "    tf.compat.v1.summary.histogram('layer_3',layer_3)\n",
        "    flat = flatten(layer_3)\n",
        "    fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "    tf.compat.v1.summary.histogram('fc',fc)\n",
        "    #Add final output layer\n",
        "    output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "    tf.compat.v1.summary.histogram('output',output)\n",
        "    vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
        "    #Return both variables and outputs together\n",
        "    return vars, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVc33AX8x-18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 90\n",
        "batch_size = 50\n",
        "input_shape = (None, 88, 80, 1)\n",
        "#Recall shape is img.reshape(88,80,1)\n",
        "learning_rate = 0.001\n",
        "X_shape = (None, 88, 80, 1)\n",
        "discount_factor = 0.97\n",
        "global_step = 0\n",
        "copy_steps = 10000\n",
        "steps_train = 4\n",
        "start_steps = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr_eUQS1yaUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.08\n",
        "eps_min = 0.1\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 20000\n",
        "#\n",
        "def epsilon_greedy(action, step):\n",
        "  p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "  epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps\n",
        "  if np.random.rand() < epsilon:\n",
        "    return np.random.randint(n_outputs)\n",
        "  else:\n",
        "    return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh7mnEgxy1fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "buffer_len = 20000\n",
        "#Buffer is made from a deque — double ended queue\n",
        "exp_buffer = deque(maxlen=buffer_len)\n",
        "def sample_memories(batch_size):\n",
        "  perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "  mem = np.array(exp_buffer)[perm_batch]\n",
        "  return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHo5s8sl6wU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# define the placeholder for our input i.e game state\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# define a boolean called in_training_model to toggle the training\n",
        "in_training_mode = tf.compat.v1.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jONHI0Cy6S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "# similarly we build our target Q network, for policy evaluation\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n",
        "copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92cAICKr7XBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the placeholder for  action values\n",
        "\n",
        "X_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n",
        "\n",
        "copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "220RCyObzAph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a placeholder for our output i.e action\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "loss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.compat.v1.summary.merge_all()\n",
        "file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTtXmOrU932C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_observation(obs):\n",
        "\n",
        "    # Crop and resize the image\n",
        "    img = obs[1:176:2, ::2]\n",
        "\n",
        "    # Convert the image to greyscale\n",
        "    img = img.mean(axis=2)\n",
        "\n",
        "    # Improve image contrast\n",
        "    img[img==color] = 0\n",
        "\n",
        "    # Next we normalize the image from -1 to +1\n",
        "    img = (img - 128) / 128 - 1\n",
        "\n",
        "    return img.reshape(88,80,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL-r0ubzzJB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "  init.run()\n",
        "  # for each episode\n",
        "  history = []\n",
        "  for i in range(num_episodes):\n",
        "      done = False\n",
        "      obs = env.reset()\n",
        "      epoch = 0\n",
        "      episodic_reward = 0\n",
        "      actions_counter = Counter()\n",
        "      episodic_loss = []\n",
        "    # while the state is not the terminal state\n",
        "      while not done:\n",
        "    # get the preprocessed game screen\n",
        "          obs = preprocess_observation(obs)\n",
        "    # feed the game screen and get the Q values for each action, \n",
        "          actions = mainQ_outputs.eval(feed_dict={X:[obs],     in_training_mode:False})\n",
        "\n",
        "    # get the action\n",
        "          action = np.argmax(actions, axis=-1)\n",
        "          actions_counter[str(action)] += 1\n",
        "    # select the action using epsilon greedy policy\n",
        "          action = epsilon_greedy(action, global_step)\n",
        "    # now perform the action and move to the next state, next_obs, receive reward\n",
        "          next_obs, reward, done, _ = env.step(action)\n",
        "    # Store this transition as an experience in the replay buffer! Quite important\n",
        "          exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
        "    # After certain steps we move on to generating y-values for Q network with samples from the experience replay buffer\n",
        "          if global_step % steps_train == 0 and global_step > start_steps:\n",
        "              o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "\n",
        "        # states\n",
        "              o_obs = [x for x in o_obs]\n",
        "        # next states\n",
        "              o_next_obs = [x for x in o_next_obs]\n",
        "        # next actions\n",
        "              next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "        #discounted reward for action: these are our Y-values\n",
        "              y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
        "        # merge all summaries and write to the file\n",
        "              mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "              file_writer.add_summary(mrg_summary, global_step)\n",
        "        # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "              train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "\n",
        "              episodic_loss.append(train_loss)\n",
        "  # after some interval we copy our main Q network weights to target Q network\n",
        "          if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "              copy_target_to_main.run()\n",
        "          obs = next_obs\n",
        "          epoch += 1\n",
        "          global_step += 1\n",
        "          episodic_reward += reward\n",
        "      history.append(episodic_reward)\n",
        "      print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,'Episode number:', len(history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3qphEg88NQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-sJCiYEJ4Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean(history))\n",
        "print(np.max(history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxxkIhVlzhmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utility functions to enable video recording of gym environment and displaying it. To enable video, just do “env = wrap_env(env)\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "          loop controls style=\"height: 400px;\"\">\n",
        "          <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "          </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print('Could not find video')    \n",
        "def wrap_env(env):\n",
        "\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBd2NUhdz06v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate model on openAi GYM\n",
        "env = wrap_env(gym.make('MsPacman-v0'))\n",
        "observation = env.reset()\n",
        "new_observation = observation\n",
        "prev_input = None\n",
        "done = False\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    while True:\n",
        "      if True:\n",
        "    #set input to network to be difference image\n",
        "        obs = preprocess_observation(observation)\n",
        "    # feed the game screen and get the Q values for each action\n",
        "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "    # get the action\n",
        "        action = np.argmax(actions, axis=-1)\n",
        "        actions_counter[str(action)] += 1\n",
        "    # select the action using epsilon greedy policy\n",
        "      \n",
        "        action = epsilon_greedy(action, global_step)\n",
        "        #env.render()\n",
        "        observation = new_observation\n",
        "    # now perform the action and move to the next state, next_obs, receive reward\n",
        "        new_observation, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "        #observation = env.reset()\n",
        "          break\n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}